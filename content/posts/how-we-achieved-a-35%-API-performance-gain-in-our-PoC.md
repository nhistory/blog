+++
date = '2023-01-13T20:51:45-03:00'
title = 'How We Achieved a 35% API Performance Gain in Our PoC'
+++

Our project was a used car search and comparison platform. Being a PoC, it wasn't yet facing heavy user load. However, during our internal testing and validation phases, we hit a snag: certain API calls were unexpectedly sluggish under specific conditions. This wasn't ideal, even for a PoC, as it hampered testing efficiency and could misrepresent the potential user experience during demos. The goal was clear – we needed to optimize.

## The Problem: Sluggish Responses Under Specific Test Scenarios

The main culprit was our primary search API. While simple searches were fast, we discovered that combining multiple specific filters – for instance, filtering by make, model range, specific year, mileage bracket, and price range simultaneously – caused response times to jump significantly. It wasn't about the number of users, but the complexity of certain queries during our rigorous testing cycles.

### This slowdown was a concern because:

- It made iterative testing and debugging cycles longer than necessary.
- It could potentially skew perceptions of the platform's viability during internal reviews or demos.
- It hinted at underlying inefficiencies that would become major problems if the PoC moved towards production.

## Digging Deep: Identifying the Bottlenecks

### To pinpoint the cause, we employed a few standard diagnostic techniques:

- **Profiling**: We used Python's built-in [cProfile](https://docs.python.org/3/library/profile.html#module-cProfile) and tools like [py-spy](https://github.com/benfred/py-spy) to analyze the execution time of the code within the problematic API endpoints. This helped us see where the application was spending most of its time within the Python code itself.
- **Logging & Timing Middleware**: We added simple middleware in FastAPI to log the precise time taken for each request, especially focusing on the slow scenarios identified during testing.
- **Database Query Analysis (MongoDB)**: Since we used MongoDB, we heavily utilized the explain() command on the queries generated by our application (likely via an async driver like Motor). This showed us the query execution plan, index usage (or lack thereof), and documents scanned.

### Our investigation revealed a couple of key areas contributing to the slowdown:

- **Inefficient Database Queries**: The complex filter combinations often resulted in queries that couldn't effectively use existing indexes, leading to collection scans or inefficient index scans in MongoDB.
- **Suboptimal Data Handling in Code**: In some cases, the Python code was fetching more data than necessary from the database and then performing complex filtering or transformations in application memory, which was less efficient than doing it at the database level.
- **Serialization Overhead**: For complex results, serializing large Pydantic models into JSON added a noticeable delay, especially when unnecessary fields were included.

## The Fixes: Implementing Targeted Optimizations

Based on our findings, we implemented the following changes, which cumulatively led to the ~35% average performance improvement for those specific problematic scenarios:

1. Strategic Database Indexing & Query Refinement (MongoDB):

- **Issue**: explain() showed that queries combining make, model, year, and price_range weren't using indexes well.

- **Solution**:

  - We created compound indexes specifically designed to cover the most common complex filter combinations identified during testing. For example: db.cars.create_index([("make", 1), ("model", 1), ("year", -1), ("price", 1)]).
  - We refactored our database query logic (using Motor) to leverage MongoDB's aggregation pipeline for some complex filtering/grouping tasks, pushing more work onto the database engine which is optimized for it.
  - We explicitly used projections (find({}, {"\_id": 0, "name": 1, "price": 1, "year": 1})) in our queries to ensure we only retrieved the fields actually needed by the API endpoint, reducing data transfer and processing load.

- **Impact**: This was the most significant contributor. Query times for the complex filter scenarios dropped dramatically, often by more than 50% in isolation.

2. Optimizing Data Processing & Serialization:

- **Issue**: Profiling showed time spent processing fetched data and serializing large Pydantic models.

- **Solution**:

  - We reviewed the API logic. Where possible, filtering logic previously done in Python after fetching a broader dataset was pushed down into the optimized database query itself.
  - We created specific Pydantic response_model schemas tailored for different API use cases. For the list view endpoint (which was often the slow one with filters), we created a slimmer model containing only essential fields (e.g., CarListModel), distinct from a more detailed CarDetailModel. FastAPI uses this hint to optimize serialization.

- **Impact**: Reduced CPU usage for data manipulation within Python and significantly cut down on JSON serialization time, especially for requests returning multiple items.

(Note: While async I/O is crucial for FastAPI, in this specific PoC scenario, the bottleneck wasn't primarily external I/O blocking, but rather database query and data processing efficiency under complex filter conditions. If external API calls were involved, optimizing them with httpx and async/await would have been another key step.)

## Measuring the Improvement

How did we verify the 35% improvement? Since it was a PoC and the issue appeared under specific test conditions, we focused on benchmarking those scenarios:

- **Baseline Tests**: We wrote scripts (using pytest and httpx, or even simple curl loops) to repeatedly hit the API endpoint with the exact filter combinations that we knew were slow, measuring the average response time before optimization.
- **Post-Optimization Tests**: We ran the exact same test scripts after implementing the fixes.
- **Comparison**: Comparing the "before" and "after" average response times for these targeted, complex queries showed an average reduction of approximately 35%.

We also kept an eye on profiling results and database query explain() outputs to confirm the reasons for the speedup (e.g., better index usage, fewer documents scanned).

## Conclusion

Even in a Proof of Concept stage, API performance matters. Discovering and addressing bottlenecks early, as we did during testing, prevents these issues from becoming major roadblocks later. For our FastAPI application, the key lay in deeply analyzing the behavior under specific stressful conditions (complex filters in our case) and applying targeted optimizations, primarily focused on database interaction (indexing, query structure, projections) and efficient data handling/serialization within the application code.

This iterative process of testing, profiling, optimizing, and re-testing allowed us to achieve that significant 35% performance gain, making our PoC more robust and efficient even before facing real-world traffic.
